image_recognition_tensorflow.MovingAvgQuantize
==============================================

.. py:module:: image_recognition_tensorflow.MovingAvgQuantize

.. autoapi-nested-parse::

   Copied from https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/contrib/quantize/python/quant_ops.py



Functions
---------

.. autoapisummary::

   image_recognition_tensorflow.MovingAvgQuantize._ModelVariable
   image_recognition_tensorflow.MovingAvgQuantize.MovingAvgQuantize
   image_recognition_tensorflow.MovingAvgQuantize._FakeQuantWithMinMaxVars


Module Contents
---------------

.. py:function:: _ModelVariable(name, shape=None, initializer=None, collections=None, trainable=None)

.. py:function:: MovingAvgQuantize(inputs, per_channel=False, init_min=-6.0, init_max=6.0, ema_decay=0.999, vars_collection=ops.GraphKeys.MOVING_AVERAGE_VARIABLES, name_prefix='MovingAvgQuantize', reuse=None, is_training=True, num_bits=8, narrow_range=False, symmetric=False)

   Adds a layer that collects quantization ranges as EMAs of input ranges.
   MovingAvgQuantize creates variables called 'min' and 'max', representing the
   interval used for quantization and clamping.
   :param inputs: a tensor containing values to be quantized.
   :param per_channel: (default False) a boolean specifying whether to use different
                       quantization ranges per output channel.
   :param init_min: a float scalar, the initial value for variable min.
   :param init_max: a float scalar, the initial value for variable max.
   :param ema_decay: EMA decay parameter.
   :param vars_collection: (Optional) collection where to store variables for
                           quantization interval ends.
   :param name_prefix: name_prefix for created nodes.
   :param reuse: whether or not the layer and its variables should be reused. To be
                 able to reuse the layer scope must be given.
   :param is_training: Whether the op is applied to a training or eval graph.
   :param num_bits: Number of bits to use for quantization, must be between 2 and 8.
   :param narrow_range: Whether to use the narrow quantization range
                        [1; 2^num_bits - 1] or wide range [0; 2^num_bits - 1].
   :param symmetric: If true, use symmetric quantization limits instead of training
                     the minimum and maximum of each quantization range separately.

   :returns: a tensor containing quantized values.


.. py:function:: _FakeQuantWithMinMaxVars(inputs, min_var, max_var, per_channel, num_bits, narrow_range)

   Adds a fake quantization operation.
   Depending on value of per_channel, this operation may do global quantization
   or per channel quantization.  min_var and max_var should have corresponding
   shapes: [1] when per_channel == False and [d] when per_channel == True.
   :param inputs: a tensor containing values to be quantized.
   :param min_var: a variable containing quantization range lower end(s).
   :param max_var: a variable containing quantization range upper end(s).
   :param per_channel: a boolean specifying whether to use per-channel quantization.
   :param num_bits: Number of bits to use for quantization, must be between 2 and 8.
   :param narrow_range: Whether to use the narrow quantization range
                        [1; 2^num_bits - 1] or wide range [0; 2^num_bits - 1].

   :returns: a tensor containing quantized values.


